{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and data paths\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '../data')\n",
    "export_dir = os.path.join(current_dir, 'models')\n",
    "\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "CLF_EMBODIED_CARBON_PATH = os.path.join(data_dir, 'CLF Embodied Carbon_Cleaned.csv')\n",
    "CARB_EN_MATS_PATH = os.path.join(data_dir, 'CarbEnMats_dataset.xlsx')\n",
    "BECD_PATH = os.path.join(data_dir, 'BECD_2024-06-17 18.41.17.csv')\n",
    "\n",
    "clf_df = pd.read_csv(CLF_EMBODIED_CARBON_PATH)\n",
    "carbenmats_df = pd.read_excel(CARB_EN_MATS_PATH)\n",
    "becd_df = pd.read_csv(BECD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align and Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total embodied carbon for BECD\n",
    "becd_df['Total_Embodied_Carbon'] = becd_df[\n",
    "    ['Total_Normalised_A1ToA3', 'Total_Normalised_A4', 'Total_Normalised_A5',\n",
    "     'Total_Normalised_B1', 'Total_Normalised_B2', 'Total_Normalised_B3',\n",
    "     'Total_Normalised_B4', 'Total_Normalised_B5', 'Total_Normalised_C1',\n",
    "     'Total_Normalised_C2', 'Total_Normalised_C3', 'Total_Normalised_C4',\n",
    "     'Total_Normalised_D']\n",
    "].sum(axis=1)\n",
    "\n",
    "# Calculate the total embodied carbon for CarbEnMats\n",
    "carbenmats_df['Total_Embodied_Carbon'] = carbenmats_df[\n",
    "    ['GHG_A123_m2a', 'GHG_A45_m2a', 'GHG_B1234_m2a', 'GHG_B5_m2a',\n",
    "     'GHG_B67_m2a', 'GHG_C12_m2a', 'GHG_C34_m2a', 'GHG_D_m2a']\n",
    "].sum(axis=1)\n",
    "\n",
    "# Function to split the interval\n",
    "def split_interval(interval):\n",
    "    if interval == 'No data':\n",
    "        return pd.Series([None, None])\n",
    "    try:\n",
    "        min_val, max_val = map(int, interval.split('-'))\n",
    "        return pd.Series([min_val, max_val])\n",
    "    except ValueError:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Rename columns to align with clf_df\n",
    "becd_df.rename(columns={'EntityCode': 'Building Public ID', 'ProjectType': 'Building New or Renovation',\n",
    "                        'Location': 'Building Location Region', 'SizePrimary': 'BuildingAreaExact SquareMeters', \n",
    "                        'Total_Normalised_TotalBiogenicCarbon': 'Total_Biogenic_Carbon'}, inplace=True) \n",
    "        # need to also change all new/reno names correctly.\n",
    "\n",
    "carbenmats_df.rename(columns={'site_country': 'Country', 'bldg_use_type': 'Building Type', 'bldg_use_subtype': 'Building Use', \n",
    "                              'site_region_world': 'Building Location Region', 'bldg_project_type': 'Building New or Renovation' }, inplace=True)\n",
    "carbenmats_df[['Minimum Building Area in Square Meters', 'Maximum Building Area in Square Meters']] = carbenmats_df['bldg_area_interval'].apply(split_interval)\n",
    "carbenmats_df[['Minimum Building Storeys', 'Maximum Building Storeys']] = carbenmats_df['bldg_floors_ag_interval'].apply(split_interval)\n",
    "        # need to change all non-residential in building use type to 'commercial'\n",
    "        # need to change all \"multi-family house....\" to just \"multi-family\" in building use, do the same for single-family also\n",
    "        #remove any \"No data\" etc\n",
    "\n",
    "clf_df.rename(columns={'Embodied Carbon Whole Building Excluding Operational': 'Total_Embodied_Carbon'}, inplace=True)\n",
    "# change building location region to continents\n",
    "\n",
    "# Define a function to combine columns\n",
    "def combine_columns(df, column_map):\n",
    "    for new_col, old_cols in column_map.items():\n",
    "        if old_cols:  # Ensure there's something to sum\n",
    "            df[new_col] = df[old_cols].sum(axis=1)\n",
    "        else:  # If no columns to sum, create the column with NaNs\n",
    "            df[new_col] = pd.Series([float('nan')] * len(df), index=df.index)\n",
    "    return df\n",
    "\n",
    "# Define the column mappings\n",
    "becd_column_map = {\n",
    "    'A123': ['Total_Normalised_A1ToA3'],\n",
    "    'A45': ['Total_Normalised_A4', 'Total_Normalised_A5'],\n",
    "    'B1234': ['Total_Normalised_B1', 'Total_Normalised_B2', 'Total_Normalised_B3', 'Total_Normalised_B4'],\n",
    "    'B5': ['Total_Normalised_B5'],\n",
    "    'B67': [],  # No equivalent in BECD\n",
    "    'C12': ['Total_Normalised_C1', 'Total_Normalised_C2'],\n",
    "    'C34': ['Total_Normalised_C3', 'Total_Normalised_C4'],\n",
    "    'D': ['Total_Normalised_D']\n",
    "}\n",
    "\n",
    "carbenmats_column_map = {\n",
    "    'A123': ['GHG_A123_m2a'],\n",
    "    'A45': ['GHG_A45_m2a'],\n",
    "    'B1234': ['GHG_B1234_m2a'],\n",
    "    'B5': ['GHG_B5_m2a'],\n",
    "    'B67': ['GHG_B67_m2a'],\n",
    "    'C12': ['GHG_C12_m2a'],\n",
    "    'C34': ['GHG_C34_m2a'],\n",
    "    'D': ['GHG_D_m2a']\n",
    "}\n",
    "\n",
    "# Apply the column mappings to create combined columns\n",
    "becd_df_combined = combine_columns(becd_df.copy(), becd_column_map)\n",
    "carbenmats_df_combined = combine_columns(carbenmats_df.copy(), carbenmats_column_map)\n",
    "\n",
    "# Align columns\n",
    "all_columns = set(becd_df_combined.columns).union(set(carbenmats_df_combined.columns)).union(set(clf_df.columns))\n",
    "becd_df_combined = becd_df_combined.reindex(columns=all_columns)\n",
    "carbenmats_df_combined = carbenmats_df_combined.reindex(columns=all_columns)\n",
    "clf_df = clf_df.reindex(columns=all_columns)\n",
    "\n",
    "# Add a column to indicate the dataset source\n",
    "becd_df_combined['Dataset'] = 'BECD'\n",
    "carbenmats_df_combined['Dataset'] = 'CarbEnMats'\n",
    "clf_df['Dataset'] = 'CLF'\n",
    "\n",
    "# Concatenate dataframes\n",
    "merged_df = pd.concat([becd_df_combined, carbenmats_df_combined, clf_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select and Rename Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main dataframe\n",
    "df = merged_df[['Dataset', 'Country', 'Building Location Region', 'Building Type', 'Building Use', 'Building New or Renovation', 'bldg_users_total', \n",
    "                       'Minimum Building Area in Square Meters', 'Maximum Building Area in Square Meters', 'BuildingAreaExact SquareMeters',\n",
    "                       'Minimum Building Storeys', 'Maximum Building Storeys', 'AboveGroundStorey', 'UndergroundStorey', 'Total_Biogenic_Carbon',\n",
    "\n",
    "                       'mass_wood', 'mass_straw_hemp', 'mass_fungi', 'mass_brass_copper', 'mass_earth',\n",
    "                       'mass_bamboo', 'mass_glass', 'mass_stone', 'mass_stone_wool', 'mass_ceramics',\n",
    "                       'mass_metals', 'mass_plastics', 'mass_steel_reinforcement', 'mass_EPS_XPS', 'mass_aluminium', \n",
    "                       'mass_concrete_wo_reinforcement', 'mass_other', 'mass_concrete_reinforced', 'mass_cement_mortar', 'mass_total_mats'\n",
    ",\n",
    "                       'PSCHorizontalElementTypePrimary', 'PSCVerticalElementStructureTypePrimary', 'PSCFinishesTypePrimary', \n",
    "                       'PSCCladdingTypePrimary', 'PSCFoundationTypePrimary', 'PSCGroundFloorTypePrimary', \n",
    "                       'PSCHeatingTypePrimary', 'PSCCoolingTypePrimary', 'PSCVentilationTypePrimary',\n",
    "                       \n",
    "                       'Total_Embodied_Carbon']]\n",
    "\n",
    "# Rename columns to be clearer\n",
    "df = df.rename(columns={\n",
    "    'Country': 'Country_Name',\n",
    "    'Building Location Region': 'Country_Region',\n",
    "    'Building Type': 'Bldg_Type',\n",
    "    'Building Use': 'Bldg_Use',\n",
    "    'Building New or Renovation': 'Construction_Type',\n",
    "    'bldg_users_total': 'Total_Bldg_Users',\n",
    "    'AboveGroundStorey': 'Exact_Storeys',\n",
    "    'UndergroundStorey': 'Underground_Storeys',\n",
    "    'Minimum Building Area in Square Meters': 'Min_Area_SqMeters',\n",
    "    'Maximum Building Area in Square Meters': 'Max_Area_SqMeters',\n",
    "    'BuildingAreaExact SquareMeters': 'Exact_Area_SqMeters',\n",
    "    'Minimum Building Storeys': 'Min_Storeys',\n",
    "    'Maximum Building Storeys': 'Max_Storeys',\n",
    "    'Total_Biogenic_Carbon': 'Total_Sequestered_Carbon',\n",
    "    'mass_wood': 'Mass_Wood',\n",
    "    'mass_straw_hemp': 'Mass_Straw_Hemp',\n",
    "    'mass_fungi': 'Mass_Fungi',\n",
    "    'mass_brass_copper': 'Mass_Brass_Copper',\n",
    "    'mass_earth': 'Mass_Earth',\n",
    "    'mass_bamboo': 'Mass_Bamboo',\n",
    "    'mass_glass': 'Mass_Glass',\n",
    "    'mass_stone': 'Mass_Stone',\n",
    "    'mass_stone_wool': 'Mass_Stone_Wool',\n",
    "    'mass_total_mats': 'Total_Materials_Mass',\n",
    "    'mass_ceramics': 'Mass_Ceramics',\n",
    "    'mass_metals': 'Mass_Metals',\n",
    "    'mass_plastics': 'Mass_Plastics',\n",
    "    'mass_steel_reinforcement': 'Mass_Steel_Reinforcement',\n",
    "    'mass_EPS_XPS': 'Mass_EPS_XPS',\n",
    "    'mass_aluminium': 'Mass_Aluminium',\n",
    "    'mass_concrete_wo_reinforcement': 'Mass_Concrete_Without_Reinforcement',\n",
    "    'mass_other': 'Mass_Other',\n",
    "    'mass_concrete_reinforced': 'Mass_Reinforced_Concrete',\n",
    "    'mass_cement_mortar': 'Mass_Cement_Mortar',\n",
    "    'PSCHorizontalElementTypePrimary': 'Primary_Horizontal_Element_Type',\n",
    "    'PSCVerticalElementStructureTypePrimary': 'Primary_Vertical_Element_Type',\n",
    "    'PSCFinishesTypePrimary': 'Primary_Finishes_Type',\n",
    "    'PSCCladdingTypePrimary': 'Primary_Cladding_Type',\n",
    "    'PSCFoundationTypePrimary': 'Primary_Foundation_Type',\n",
    "    'PSCGroundFloorTypePrimary': 'Primary_Ground_Floor_Type',\n",
    "    'PSCHeatingTypePrimary': 'Primary_Heating_Type',\n",
    "    'PSCCoolingTypePrimary': 'Primary_Cooling_Type',\n",
    "    'PSCVentilationTypePrimary': 'Primary_Ventilation_Type',\n",
    "    'Total_Embodied_Carbon': 'Total_Embodied_Carbon'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning:\n",
    "\n",
    "\n",
    "######\n",
    "# 1. Impute countries, taking region into account.\n",
    "# 2. Find region from countries.\n",
    "\n",
    "COUNTRIES = os.path.join(data_dir, 'countries_continents.csv')\n",
    "country_df = pd.read_csv(COUNTRIES)\n",
    "country_df = country_df.rename(columns={'Country': 'Country_Name'})\n",
    "\n",
    "df['Country_Name'] = df['Country_Name'].str.lower()\n",
    "country_df['Country_Name'] = country_df['Country_Name'].str.lower()\n",
    "\n",
    "# Function to capitalize the first letter of each word\n",
    "def capitalize_words(name):\n",
    "    if pd.isnull(name):\n",
    "        return None\n",
    "    return ' '.join(word.capitalize() for word in name.split())\n",
    "\n",
    "df['Country_Name'] = df['Country_Name'].apply(capitalize_words)                     # Apply the function to the 'Country_Name' columns\n",
    "country_df['Country_Name'] = country_df['Country_Name'].apply(capitalize_words)\n",
    "df['Country_Name'] = df['Country_Name'].replace('Usa', 'United States')             # Rename \"Usa\" to \"United States\"\n",
    "\n",
    "regions = country_df['Continent'].unique()                                          # Identify rows where 'Country_Name' matches any 'Country_Region'\n",
    "\n",
    "def move_region_to_country(row):\n",
    "    if row['Country_Name'] in regions:\n",
    "        if pd.isnull(row['Country_Region']):\n",
    "            row['Country_Region'] = row['Country_Name']\n",
    "        row['Country_Name'] = None\n",
    "    return row\n",
    "\n",
    "df = df.apply(move_region_to_country, axis=1)                                       # Apply the function to move region names\n",
    "region_mapping = {                                                                  # Dictionary to standardize country regions\n",
    "    'Asia-Pacific': 'Asia',\n",
    "    'Middle East, North Africa, and Greater Arabia': 'Middle East',\n",
    "    'Europe': 'Europe',\n",
    "    'North America': 'North America',\n",
    "    'South America': 'South America',\n",
    "    'Oceania': 'Oceania',\n",
    "    'Africa': 'Africa',\n",
    "    'Middle East': 'Middle East'\n",
    "}\n",
    "df['Country_Region'] = df['Country_Region'].replace(region_mapping)                 # Standardize the 'Country_Region' column\n",
    "\n",
    "df = df.merge(country_df, on='Country_Name', how='left')                            # Merge the DataFrames\n",
    "df['Country_Region'] = df['Country_Region'].combine_first(df['Continent'])          # Replace NaNs in 'Country_Region' with values from 'Country_Region_x' and 'Country_Region_y'\n",
    "df = df.drop(columns=['Continent'])                                                 # Drop unnecessary columns\n",
    "\n",
    "\n",
    "######\n",
    "# 3. Change all \"Non-residential\" to \"Commercial\"\n",
    "df['Bldg_Type'] = df['Bldg_Type'].replace('Non-residential', 'Commercial')\n",
    "def update_bldg_type(row):                                                          # Function to update Bldg_Type based on Bldg_Use\n",
    "    commercial_uses = [\n",
    "        'Park', 'Parking', 'Public Assembly', 'Public Order and Safety', \n",
    "        'Industrial', 'Office', 'Retail', 'Service', 'Mixed Use'\n",
    "    ]\n",
    "    if row['Bldg_Use'] in commercial_uses:\n",
    "        row['Bldg_Type'] = 'Commercial'\n",
    "    elif row['Bldg_Use'] == 'Residential':\n",
    "        row['Bldg_Type'] = 'Residential'\n",
    "    elif row['Bldg_Type'] in ['NonCommercial', 'Generic', 'No data']:\n",
    "        row['Bldg_Type'] = None\n",
    "    return row\n",
    "df = df.apply(update_bldg_type, axis=1)                                             # Apply the function to update Bldg_Type\n",
    "\n",
    "renovation_values = [                                                               # Mapping for Construction_Type column\n",
    "    'Existing building', 'Extension and retrofit', \n",
    "    'Refurbishment', 'Renovation', 'Retrofit'\n",
    "]\n",
    "new_values = ['New', 'New built', 'New construction', 'Demolish and new build']\n",
    "def update_construction_type(value):\n",
    "    if value in renovation_values:\n",
    "        return 'Renovation'\n",
    "    elif value in new_values:\n",
    "        return 'New'\n",
    "    else:\n",
    "        return None\n",
    "df['Construction_Type'] = df['Construction_Type'].apply(update_construction_type)    # Apply the function to update Construction_Type\n",
    "\n",
    "\n",
    "#######\n",
    "# 4. Add higher band for inf in max storeys and area, use avg of minmax for missing exact vals\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "df['Min_Storeys'] = pd.to_numeric(df['Min_Storeys'], errors='coerce')\n",
    "df['Max_Storeys'] = pd.to_numeric(df['Max_Storeys'], errors='coerce')\n",
    "df['Exact_Storeys'] = pd.to_numeric(df['Exact_Storeys'], errors='coerce')\n",
    "df['Min_Area_SqMeters'] = pd.to_numeric(df['Min_Area_SqMeters'], errors='coerce')\n",
    "df['Max_Area_SqMeters'] = pd.to_numeric(df['Max_Area_SqMeters'], errors='coerce')\n",
    "df['Exact_Area_SqMeters'] = pd.to_numeric(df['Exact_Area_SqMeters'], errors='coerce')\n",
    "df['Underground_Storeys'] = pd.to_numeric(df['Underground_Storeys'], errors='coerce')\n",
    "\n",
    "# Replace 'inf' in Max_Storeys column with 100\n",
    "df['Max_Storeys'] = df['Max_Storeys'].replace([np.inf, 'inf'], 100)\n",
    "\n",
    "# Replace 'inf' in Max_Area_SqMeters column with 100\n",
    "df['Max_Area_SqMeters'] = df['Max_Area_SqMeters'].replace([np.inf, 'inf'], 1000000)\n",
    "\n",
    "# Replace values in Exact_Storeys greater than 200 with NaN\n",
    "df.loc[df['Exact_Storeys'] > 200, 'Exact_Storeys'] = np.nan\n",
    "# Replace values in Underground_Storeys greater than 20 with NaN\n",
    "df.loc[df['Underground_Storeys'] > 20, 'Underground_Storeys'] = np.nan\n",
    "\n",
    "# Fill missing Exact_Storeys with the median of Min_Storeys and Max_Storeys\n",
    "df['Exact_Storeys'] = df.apply(\n",
    "    lambda row: (row['Min_Storeys'] + row['Max_Storeys']) / 2 \n",
    "    if np.isnan(row['Exact_Storeys']) else row['Exact_Storeys'], axis=1\n",
    "    )\n",
    "# Fill missing Exact_Area_SqMeters with the median of Min_Area_SqMeters and Max_Area_SqMeters\n",
    "df['Exact_Area_SqMeters'] = df.apply(\n",
    "    lambda row: (row['Min_Area_SqMeters'] + row['Max_Area_SqMeters']) / 2 \n",
    "    if np.isnan(row['Exact_Area_SqMeters']) else row['Exact_Area_SqMeters'], axis=1\n",
    "    )\n",
    "\n",
    "# Replace 0s in Max_Storeys column with nan\n",
    "df['Exact_Storeys'] = df['Exact_Storeys'].replace(0, np.nan)\n",
    "\n",
    "# Drop Min and Max columns\n",
    "df.drop(columns=['Min_Storeys', 'Max_Storeys', 'Min_Area_SqMeters', 'Max_Area_SqMeters'], inplace=True)\n",
    "\n",
    "\n",
    "#######\n",
    "# 5. Remove all entries that are \"N/A\" or \"No data\"\n",
    "\n",
    "# List of values to be replaced with empty strings\n",
    "values_to_replace = [\"No data\", \"no data\", \"Not applicable\", \"Not Applicable\", \"NA\", \"N/A\", \"Unknown\"]\n",
    "# Replace the specified values with empty strings\n",
    "df.replace(values_to_replace, '', inplace=True)                                       \n",
    "\n",
    "\n",
    "######\n",
    "# 6. For all masses that don't have a value, input 0.\n",
    "mass_columns = ['Mass_Wood', 'Mass_Straw_Hemp', 'Mass_Fungi', 'Mass_Brass_Copper', 'Mass_Earth', \n",
    "                'Mass_Bamboo', 'Mass_Glass', 'Mass_Stone', 'Mass_Stone_Wool', 'Mass_Ceramics',\n",
    "                'Mass_Metals', 'Mass_Plastics', 'Mass_Steel_Reinforcement', 'Mass_EPS_XPS', 'Mass_Aluminium',\n",
    "                'Mass_Concrete_Without_Reinforcement', 'Mass_Other', 'Mass_Reinforced_Concrete', 'Mass_Cement_Mortar', \n",
    "                'Total_Materials_Mass']\n",
    "# Fill missing mass values with zero\n",
    "df[mass_columns] = df[mass_columns].fillna(0)\n",
    "\n",
    "# Identify rows where all mass columns are zero\n",
    "all_mass_zero = df[mass_columns].sum(axis=1) == 0\n",
    "\n",
    "# Set mass values to NaN for rows where all mass columns are zero\n",
    "df.loc[all_mass_zero, mass_columns] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# 7.  Any biogenic with no data should be 0. Any carbon stages with no data should be 0.\n",
    "biogenic_columns = ['Total_Sequestered_Carbon']\n",
    "\n",
    "# Function to replace \"no data\" values with 0\n",
    "def replace_no_data_with_zero(column):                                                \n",
    "    df[column] = df[column].replace(['No data', 'no data', 'Not applicable', 'Not Applicable', 'NA', 'N/A', None, np.nan], 0).astype(float)\n",
    "# Replace \"no data\" values with 0 for biogenic columns\n",
    "for column in biogenic_columns:\n",
    "    replace_no_data_with_zero(column)\n",
    "\n",
    "######\n",
    "# 8. Total carbon should include total sequestered (total accrued - total sequestered = actual total)\n",
    "# Make all sequestered carbon values negative\n",
    "df['Total_Sequestered_Carbon'] = df['Total_Sequestered_Carbon'].apply(lambda x: -abs(x))\n",
    "# Calculate the actual total carbon\n",
    "df['Actual_Total_Carbon'] = df['Total_Embodied_Carbon'] + df['Total_Sequestered_Carbon']\n",
    "# Drop sequestered carbon column\n",
    "df.drop(columns=['Total_Sequestered_Carbon'], inplace=True)\n",
    "\n",
    "\n",
    "######\n",
    "# 9. Remove all entries with total carbon as 0.\n",
    "df = df[df['Actual_Total_Carbon'] != 0]\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# 10. Remove outliers from dataset\n",
    "\n",
    "# Define a function to identify outliers using Z-score\n",
    "def identify_outliers_zscore(data, threshold=3):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    z_scores = [(x - mean) / std_dev for x in data]\n",
    "    return np.where(np.abs(z_scores) > threshold)\n",
    "\n",
    "# Identify outliers\n",
    "outlier_indices = identify_outliers_zscore(df['Actual_Total_Carbon'], threshold=3)\n",
    "\n",
    "# Print the outliers\n",
    "outliers = df.iloc[outlier_indices]\n",
    "#print(\"Outliers identified using Z-score method:\")\n",
    "#print(outliers)\n",
    "\n",
    "# Remove outliers using a boolean mask\n",
    "mask = np.ones(len(df), dtype=bool)\n",
    "mask[outlier_indices] = False\n",
    "df = df[mask]\n",
    "\n",
    "\n",
    "# Drop the 'Total_Embodied_Carbon' column if no longer needed\n",
    "df = df.drop(columns=['Total_Embodied_Carbon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2736 entries, 0 to 3571\n",
      "Data columns (total 40 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   Dataset                              2736 non-null   object \n",
      " 1   Country_Name                         1404 non-null   object \n",
      " 2   Country_Region                       2442 non-null   object \n",
      " 3   Bldg_Type                            1881 non-null   object \n",
      " 4   Bldg_Use                             1880 non-null   object \n",
      " 5   Construction_Type                    2161 non-null   object \n",
      " 6   Total_Bldg_Users                     388 non-null    float64\n",
      " 7   Exact_Area_SqMeters                  2497 non-null   float64\n",
      " 8   Exact_Storeys                        1824 non-null   float64\n",
      " 9   Underground_Storeys                  814 non-null    float64\n",
      " 10  Mass_Wood                            131 non-null    float64\n",
      " 11  Mass_Straw_Hemp                      131 non-null    float64\n",
      " 12  Mass_Fungi                           131 non-null    float64\n",
      " 13  Mass_Brass_Copper                    131 non-null    float64\n",
      " 14  Mass_Earth                           131 non-null    float64\n",
      " 15  Mass_Bamboo                          131 non-null    float64\n",
      " 16  Mass_Glass                           131 non-null    float64\n",
      " 17  Mass_Stone                           131 non-null    float64\n",
      " 18  Mass_Stone_Wool                      131 non-null    float64\n",
      " 19  Mass_Ceramics                        131 non-null    float64\n",
      " 20  Mass_Metals                          131 non-null    float64\n",
      " 21  Mass_Plastics                        131 non-null    float64\n",
      " 22  Mass_Steel_Reinforcement             131 non-null    float64\n",
      " 23  Mass_EPS_XPS                         131 non-null    float64\n",
      " 24  Mass_Aluminium                       131 non-null    float64\n",
      " 25  Mass_Concrete_Without_Reinforcement  131 non-null    float64\n",
      " 26  Mass_Other                           131 non-null    float64\n",
      " 27  Mass_Reinforced_Concrete             131 non-null    float64\n",
      " 28  Mass_Cement_Mortar                   131 non-null    float64\n",
      " 29  Total_Materials_Mass                 131 non-null    float64\n",
      " 30  Primary_Horizontal_Element_Type      788 non-null    object \n",
      " 31  Primary_Vertical_Element_Type        788 non-null    object \n",
      " 32  Primary_Finishes_Type                788 non-null    object \n",
      " 33  Primary_Cladding_Type                788 non-null    object \n",
      " 34  Primary_Foundation_Type              788 non-null    object \n",
      " 35  Primary_Ground_Floor_Type            788 non-null    object \n",
      " 36  Primary_Heating_Type                 788 non-null    object \n",
      " 37  Primary_Cooling_Type                 788 non-null    object \n",
      " 38  Primary_Ventilation_Type             788 non-null    object \n",
      " 39  Actual_Total_Carbon                  2707 non-null   float64\n",
      "dtypes: float64(25), object(15)\n",
      "memory usage: 876.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Save the merged dataframe to a CSV file\n",
    "DF_PATH = os.path.join(export_dir, 'BUILDING_DATA.csv')\n",
    "df.to_csv(DF_PATH, index=False)\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
